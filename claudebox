#!/usr/bin/env bash
set -euo pipefail

# Configuration
DEFAULT_FLAGS=()
readonly IMAGE_NAME="claudebox"
readonly DOCKER_USER="claude"
readonly USER_ID=$(id -u)
readonly GROUP_ID=$(id -g)
readonly PROJECT_DIR="$(pwd)"

# Cross-platform script path resolution
get_script_path() {
    local source="${BASH_SOURCE[0]:-$0}"
    while [[ -L "$source" ]]; do
        local dir="$(cd -P "$(dirname "$source")" && pwd)"
        source="$(readlink "$source")"
        [[ $source != /* ]] && source="$dir/$source"
    done
    echo "$(cd -P "$(dirname "$source")" && pwd)/$(basename "$source")"
}
readonly SCRIPT_PATH="$(get_script_path)"

readonly CLAUDE_DATA_DIR="$HOME/.claude"
readonly LINK_TARGET="$HOME/.local/bin/claudebox"
readonly NODE_VERSION="--lts"

# Default flags for Claude Code - comment/uncomment as needed
DEFAULT_FLAGS+=("--dangerously-skip-permissions")

# Default ClaudeBox flags - comment/uncomment as needed  
CLAUDEBOX_ENABLE_SUDO=true

# Color codes
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly PURPLE='\033[0;35m'
readonly CYAN='\033[0;36m'
readonly WHITE='\033[1;37m'
readonly NC='\033[0m'

# Utility functions
cecho() { echo -e "${2:-$NC}$1${NC}"; }
error() { cecho "$1" "$RED" >&2; exit "${2:-1}"; }
warn() { cecho "$1" "$YELLOW"; }
info() { cecho "$1" "$BLUE"; }
success() { cecho "$1" "$GREEN"; }

# Parse early flags
VERBOSE=false
for arg in "$@"; do
    case "$arg" in
        -v|--verbose) VERBOSE=true ;;
    esac
done

# Logo
logo() {
    local cb='
 ██████╗██╗      █████╗ ██╗   ██╗██████╗ ███████╗
██╔════╝██║     ██╔══██╗██║   ██║██╔══██╗██╔════╝
██║     ██║     ███████║██║   ██║██║  ██║█████╗
██║     ██║     ██╔══██║██║   ██║██║  ██║██╔══╝
╚██████╗███████╗██║  ██║╚██████╔╝██████╔╝███████╗
 ╚═════╝╚══════╝╚═╝  ╚═╝ ╚═════╝ ╚═════╝ ╚══════╝

██████╗  ██████╗ ██╗  ██╗ ------ ┌──────────────┐
██╔══██╗██╔═══██╗╚██╗██╔╝ ------ │ The Ultimate │
██████╔╝██║   ██║ ╚███╔╝  ------ │ Claude Code  │
██╔══██╗██║   ██║ ██╔██╗  ------ │  Docker Dev  │
██████╔╝╚██████╔╝██╔╝ ██╗ ------ │ Environment  │
╚═════╝  ╚═════╝ ╚═╝  ╚═╝ ------ └──────────────┘
'
    while IFS= read -r l; do
        o="" c=""
        for ((i=0;i<${#l};i++)); do
            ch="${l:$i:1}"
            [[ "$ch" == " " ]] && { o+="$ch"; continue; }
            cc=$(printf '%d' "'$ch" 2>/dev/null||echo 0)
            if [[ $cc -ge 32 && $cc -le 126 ]]; then n='\033[33m'
            elif [[ $cc -ge 9552 && $cc -le 9580 ]]; then n='\033[34m'
            elif [[ $cc -eq 9608 ]]; then n='\033[31m'
            else n='\033[37m'; fi
            [[ "$n" != "$c" ]] && { o+="$n"; c="$n"; }
            o+="$ch"
        done
        echo -e "${o}\033[0m"
    done <<< "$cb"
}

# Symlink management
update_symlink() {
    if [[ -L "$LINK_TARGET" ]]; then
        local current_target
        current_target=$(readlink "$LINK_TARGET" 2>/dev/null || echo "")
        if [[ "$current_target" != "$SCRIPT_PATH" ]]; then
            rm -f "$LINK_TARGET"
            ln -s "$SCRIPT_PATH" "$LINK_TARGET"
            info "Updated claudebox symlink to current location"
        fi
    elif [[ ! -e "$LINK_TARGET" ]]; then
        mkdir -p "$(dirname "$LINK_TARGET")"
        if ln -s "$SCRIPT_PATH" "$LINK_TARGET" 2>/dev/null; then
            success "Created claudebox symlink in $(dirname "$LINK_TARGET")"
        else
            warn "Note: Could not create symlink (needs sudo)"
            warn "Run with sudo or add $(dirname "$LINK_TARGET") to your PATH"
        fi
    fi
}

# Docker checks
check_docker() {
    command -v docker &>/dev/null || return 1
    docker info &>/dev/null || return 2
    docker ps &>/dev/null || return 3
    return 0
}

install_docker() {
    warn "Docker is not installed."
    cecho "Would you like to install Docker now? (y/n)" "$CYAN"
    read -r response
    [[ "$response" =~ ^[Yy]$ ]] || error "Docker is required. Visit: https://docs.docker.com/engine/install/"

    info "Installing Docker..."

    [[ -f /etc/os-release ]] && . /etc/os-release || error "Cannot detect OS"

    case "${ID:-}" in
        ubuntu|debian)
            warn "Installing Docker requires sudo privileges..."
            sudo apt-get update
            sudo apt-get install -y ca-certificates curl gnupg lsb-release
            sudo mkdir -p /etc/apt/keyrings
            curl -fsSL "https://download.docker.com/linux/$ID/gpg" | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
            echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/$ID $(lsb_release -cs) stable" | \
                sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
            sudo apt-get update
            sudo apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
            ;;
        fedora|rhel|centos)
            warn "Installing Docker requires sudo privileges..."
            sudo dnf -y install dnf-plugins-core
            sudo dnf config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo
            sudo dnf install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin
            sudo systemctl start docker
            sudo systemctl enable docker
            ;;
        arch|manjaro)
            warn "Installing Docker requires sudo privileges..."
            sudo pacman -S --noconfirm docker
            sudo systemctl start docker
            sudo systemctl enable docker
            ;;
        *)
            error "Unsupported OS: ${ID:-unknown}. Visit: https://docs.docker.com/engine/install/"
            ;;
    esac

    success "Docker installed successfully!"
    configure_docker_nonroot
}

configure_docker_nonroot() {
    warn "Configuring Docker for non-root usage..."
    warn "This requires sudo to add you to the docker group..."

    getent group docker >/dev/null || sudo groupadd docker
    sudo usermod -aG docker "$USER"

    success "Docker configured for non-root usage!"
    warn "You need to log out and back in for group changes to take effect."
    warn "Or run: ${CYAN}newgrp docker"
    warn "Then run 'claudebox' again."
    info "Trying to activate docker group in current shell..."
    exec newgrp docker
}

# Spinner
show_spinner() {
    local pid=$1 msg=$2 spin='⠋⠙⠹⠸⠼⠴⠦⠧⠇⠏' i=0
    echo -n "$msg "
    while kill -0 "$pid" 2>/dev/null; do
        printf "\b%s" "${spin:i++%${#spin}:1}"
        sleep 0.1
    done
    echo -e "\b${GREEN}✓${NC}"
}

# Profile definitions
declare -A PROFILES PROFILE_DESCRIPTIONS
PROFILES[c]="build-essential gcc g++ gdb valgrind cmake ninja-build clang clang-format clang-tidy cppcheck doxygen libboost-all-dev autoconf automake libtool pkg-config libcmocka-dev libcmocka0 lcov libncurses5-dev libncursesw5-dev"
PROFILE_DESCRIPTIONS[c]="C/C++ Development (compilers, debuggers, analyzers, build tools, cmocka, coverage, ncurses)"
PROFILES[openwrt]="build-essential gcc g++ make git wget unzip sudo file python3 python3-distutils rsync libncurses5-dev zlib1g-dev gawk gettext libssl-dev xsltproc libelf-dev libtool automake autoconf ccache subversion swig time qemu-system-arm qemu-system-aarch64 qemu-system-mips qemu-system-x86 qemu-utils"
PROFILE_DESCRIPTIONS[openwrt]="OpenWRT Development (cross-compilation, QEMU, build essentials)"
PROFILES[rust]="curl build-essential pkg-config libssl-dev"
PROFILE_DESCRIPTIONS[rust]="Rust Development (cargo and rustc will be installed separately)"
PROFILES[python]="python3 python3-pip python3-venv python3-dev build-essential libffi-dev libssl-dev"
PROFILE_DESCRIPTIONS[python]="Python Development (Python 3, pip, venv, build tools)"
PROFILES[go]="wget git build-essential"
PROFILE_DESCRIPTIONS[go]="Go Development (Go will be installed separately)"
PROFILES[javascript]="build-essential python3"
PROFILE_DESCRIPTIONS[javascript]="JavaScript/TypeScript Development (Node.js, npm, yarn)"

# Docker operations
docker_exec_root() {
    docker exec -u root "$@"
}

docker_exec_user() {
    docker exec -u "$DOCKER_USER" "$@"
}

# Helper functions for project profiles
get_project_folder_name() {
    echo "$1" | sed 's|^/||; s|/|-|g'
}

get_profile_file_path() {
    local profile_dir="$HOME/.claudebox/profiles"
    mkdir -p "$profile_dir"
    echo "$profile_dir/$(echo "$PROJECT_DIR" | sed 's|/|-|g' | sed 's|^-||').ini"
}

read_profile_section() {
    local profile_file="$1"
    local section="$2"
    local result=()

    if [[ -f "$profile_file" ]] && grep -q "^\[$section\]" "$profile_file"; then
        while IFS= read -r line; do
            [[ -z "$line" || "$line" =~ ^\[.*\]$ ]] && break
            result+=("$line")
        done < <(sed -n "/^\[$section\]/,/^\[/p" "$profile_file" | tail -n +2 | grep -v '^\[')
    fi

    printf '%s\n' "${result[@]}"
}

update_profile_section() {
    local profile_file="$1"
    local section="$2"
    shift 2
    local new_items=("$@")

    # Read existing items
    local existing_items=()
    readarray -t existing_items < <(read_profile_section "$profile_file" "$section")

    # Merge with new items (avoid duplicates)
    local all_items=()
    for item in "${existing_items[@]}"; do
        [[ -n "$item" ]] && all_items+=("$item")
    done

    for item in "${new_items[@]}"; do
        local found=false
        for existing in "${all_items[@]}"; do
            [[ "$existing" == "$item" ]] && found=true && break
        done
        [[ "$found" == "false" ]] && all_items+=("$item")
    done

    # Write updated profile file
    {
        # Preserve other sections
        if [[ -f "$profile_file" ]]; then
            awk -v sect="$section" '
                BEGIN { in_section=0; skip_section=0 }
                /^\[/ {
                    if ($0 == "[" sect "]") { skip_section=1; in_section=1 }
                    else { skip_section=0; in_section=0 }
                }
                !skip_section { print }
                /^\[/ && !skip_section && in_section { in_section=0 }
            ' "$profile_file"
        fi

        # Add our section
        echo "[$section]"
        for item in "${all_items[@]}"; do
            echo "$item"
        done
        echo ""
    } > "${profile_file}.tmp" && mv "${profile_file}.tmp" "$profile_file"
}

# Project-specific folder setup
setup_project_folder() {
    local project_folder_name
    project_folder_name=$(get_project_folder_name "$PROJECT_DIR")
    PROJECT_CLAUDEBOX_DIR="$HOME/.claudebox/$project_folder_name"

    mkdir -p "$PROJECT_CLAUDEBOX_DIR/claude-config"
    mkdir -p "$PROJECT_CLAUDEBOX_DIR/memory"
    mkdir -p "$PROJECT_CLAUDEBOX_DIR/context"

    info "Created project folder: $PROJECT_CLAUDEBOX_DIR"
}

# Global MCP configuration setup
setup_global_mcp_config() {
    local mcp_file="$HOME/.claudebox/.mcp.json"
    local mcp_content='{
  "mcpServers": {
    "memory": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-memory"]
    },
    "sequential-thinking": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-sequential-thinking"]
    },
    "context7": {
      "command": "npx",
      "args": ["-y", "@upstash/context7-mcp"]
    },
    "task-master-ai": {
      "command": "npx",
      "args": ["-y", "task-master-ai"]
    },
    "playwright": {
      "command": "npx",
      "args": ["-y", "@playwright/mcp@latest"]
    }
  }
}'

    # Ensure global MCP config exists
    mkdir -p "$(dirname "$mcp_file")"
    echo "$mcp_content" > "$mcp_file"
    info "Created global MCP config"
}

# Setup Claude agent command
setup_claude_agent_command() {
    mkdir -p .claude/commands
    cat << 'EOF' > .claude/commands/agent.md
# Agentic Loop Framework
<System>
You are building an Agentic Loop that can tackle any complex task with minimal role bloat.
Core principles
1. Single-brain overview – One Orchestrator owns the big picture.
2. Few, powerful agents – Reuse the same Specialist prompt for parallelism instead of inventing many micro-roles.
3. Tight feedback – A dedicated Evaluator grades outputs (0-100) and suggests concrete fixes until quality ≥ TARGET_SCORE.
4. Shared context – Every agent receives the same `context.md` so no information is siloed.
5. Repo-aware – The Orchestrator decides whether to align to the current repo or create a generic loop.
6. Explicit imperatives – Use the labels "You Must" or "Important" for non-negotiable steps; permit extra compute with "Think hard" / "ultrathink".
</System>
<Context>
Task: <<USER_DESCRIBED_TASK>>
Repo path (if any): <<ABSOLUTE_PATH_OR_NONE>>
Desired parallelism: <<N_PARALLEL_SPECIALISTS>>  (1-3 is typical)
The Orchestrator must decide:
- Whether to specialise the workflow to this repo or keep it generic.
- How many identical Specialist instances to launch (0 = sequential).
</Context>
<Instructions>
### 0  Bootstrap
- You Must create `./docs/<TASK>/context.md` containing this entire block so all agents share it.
### 1  Orchestrator
EOF
    info "Created Claude agent command template"
}

run_docker_build() {
    info "Running docker build..."
    docker build \
        --build-arg USER_ID="$USER_ID" \
        --build-arg GROUP_ID="$GROUP_ID" \
        --build-arg USERNAME="$DOCKER_USER" \
        --build-arg NODE_VERSION="$NODE_VERSION" \
        -f "$1" -t "$IMAGE_NAME" "$PROJECT_DIR"
}

# Main execution
main() {
    update_symlink

    # Check Docker
    local docker_status
    docker_status=$(check_docker; echo $?)
    case $docker_status in
        1) install_docker ;;
        2)
            warn "Docker is installed but not running."
            warn "Starting Docker requires sudo privileges..."
            sudo systemctl start docker
            docker info &>/dev/null || error "Failed to start Docker"
            docker ps &>/dev/null || configure_docker_nonroot
            ;;
        3)
            warn "Docker requires sudo. Setting up non-root access..."
            configure_docker_nonroot
            ;;
    esac

    # Handle commands
    [[ "$VERBOSE" == "true" ]] && echo "Command: ${1:-none}" >&2
    case "${1:-}" in
        profile)
            shift
            if [[ $# -eq 0 ]]; then
                cecho "Available Profiles:" "$CYAN"
                echo
                for profile in $(printf '%s\n' "${!PROFILE_DESCRIPTIONS[@]}" | sort); do
                    echo -e "  ${GREEN}$profile${NC} - ${PROFILE_DESCRIPTIONS[$profile]}"
                done
                echo
                warn "Usage: claudebox profile <name> [<name2> ...]"
                warn "Example: claudebox profile c python web"
                exit 0
            fi

            local selected=() remaining=()
            while [[ $# -gt 0 ]]; do
                if [[ -n "${PROFILES[$1]:-}" ]]; then
                    selected+=("$1")
                    shift
                else
                    remaining=("$@")
                    break
                fi
            done

            [[ ${#selected[@]} -eq 0 ]] && error "No valid profiles specified\nRun 'claudebox profile' to see available profiles"

            local profile_file
            profile_file=$(get_profile_file_path)

            # Update profiles section
            update_profile_section "$profile_file" "profiles" "${selected[@]}"

            # Read all current profiles for display
            local all_profiles=()
            readarray -t all_profiles < <(read_profile_section "$profile_file" "profiles")

            cecho "Profile: $PROJECT_DIR" "$CYAN"
            cecho "Installing profiles: ${selected[*]}" "$PURPLE"
            if [[ ${#all_profiles[@]} -gt 0 ]]; then
                cecho "All active profiles: ${all_profiles[*]}" "$GREEN"
            fi
            echo

            # Continue to main execution only if there are remaining arguments
            if [[ ${#remaining[@]} -gt 0 ]]; then
                set -- "${remaining[@]}"
            else
                # Profile setup complete, exit
                exit 0
            fi
            ;;

        install)
            shift
            [[ $# -eq 0 ]] && error "No packages specified. Usage: claudebox install <package1> <package2> ..."

            local profile_file
            profile_file=$(get_profile_file_path)

            # Update packages section
            update_profile_section "$profile_file" "packages" "$@"

            # Read all current packages for display
            local all_packages=()
            readarray -t all_packages < <(read_profile_section "$profile_file" "packages")

            cecho "Profile: $PROJECT_DIR" "$CYAN"
            cecho "Installing packages: $*" "$PURPLE"
            if [[ ${#all_packages[@]} -gt 0 ]]; then
                cecho "All packages: ${all_packages[*]}" "$GREEN"
            fi
            echo
            ;;



        shell)
            shift
            # Just add --shell-mode to the arguments and continue
            set -- "--shell-mode" "$@"
            ;;

        clean)
            shift
            case "${1:-}" in
                --help|-h)
                    cecho "ClaudeBox Clean Options:" "$CYAN"
                    echo
                    echo -e "  ${GREEN}clean${NC}                    Remove all containers (preserves image)"
                    echo -e "  ${GREEN}clean --project${NC}          Remove current project's data and profile"
                    echo -e "  ${GREEN}clean --all${NC}              Remove everything: containers, image, cache, symlink"
                    echo -e "  ${GREEN}clean --image${NC}            Remove containers and image (preserves build cache)"
                    echo -e "  ${GREEN}clean --cache${NC}            Remove Docker build cache only"
                    echo -e "  ${GREEN}clean --volumes${NC}          Remove associated Docker volumes"
                    echo -e "  ${GREEN}clean --symlink${NC}          Remove claudebox symlink only"
                    echo -e "  ${GREEN}clean --dangling${NC}         Remove dangling images and unused containers"
                    echo -e "  ${GREEN}clean --logs${NC}             Clear Docker container logs"
                    echo -e "  ${GREEN}clean --help${NC}             Show this help message"
                    echo
                    cecho "Examples:" "$YELLOW"
                    echo "  claudebox clean              # Remove all containers"
                    echo "  claudebox clean --project    # Remove only this project's container"
                    echo "  claudebox clean --image      # Remove containers and image"
                    echo "  claudebox clean --all        # Complete cleanup and reset"
                    exit 0
                    ;;
                --all|-a)
                    warn "Complete ClaudeBox cleanup: removing containers, image, cache, and symlink..."
                    # Remove all profile files
                    rm -rf "$HOME/.claudebox/profiles"
                    # Remove any containers from this image
                    docker ps -a --filter "ancestor=$IMAGE_NAME" -q | xargs -r docker rm -f 2>/dev/null || true
                    # Remove orphaned containers from images that no longer exist
                    # This is safer as it only removes containers whose images are gone
                    docker ps -a --filter "status=exited" --format "{{.ID}} {{.Image}}" | while read id image; do
                        if ! docker image inspect "$image" >/dev/null 2>&1; then
                            docker rm -f "$id" 2>/dev/null || true
                        fi
                    done
                    # Remove image
                    docker rmi -f "$IMAGE_NAME" 2>/dev/null || true
                    # Prune build cache
                    docker builder prune -af 2>/dev/null || true
                    # Remove volumes
                    docker volume ls -q --filter "name=claudebox" | xargs -r docker volume rm 2>/dev/null || true
                    # Remove symlink
                    [[ -L "$LINK_TARGET" ]] && rm -f "$LINK_TARGET" && info "Removed claudebox symlink"
                    success "Complete cleanup finished!"
                    ;;
                --image|-i)
                    warn "Removing ClaudeBox containers and image..."
                    # Remove all profile files
                    rm -rf "$HOME/.claudebox/profiles"
                    # Remove any containers from this image
                    docker ps -a --filter "ancestor=$IMAGE_NAME" -q | xargs -r docker rm -f 2>/dev/null || true
                    # Remove orphaned containers from images that no longer exist
                    # This is safer as it only removes containers whose images are gone
                    docker ps -a --filter "status=exited" --format "{{.ID}} {{.Image}}" | while read id image; do
                        if ! docker image inspect "$image" >/dev/null 2>&1; then
                            docker rm -f "$id" 2>/dev/null || true
                        fi
                    done
                    docker rmi -f "$IMAGE_NAME" 2>/dev/null || true
                    success "Containers and image removed! Build cache preserved."
                    ;;
                --cache|-c)
                    warn "Cleaning Docker build cache..."
                    docker builder prune -af
                    success "Build cache cleaned!"
                    ;;
                --volumes|-v)
                    warn "Removing ClaudeBox-related volumes..."
                    docker volume ls -q --filter "name=claudebox" | xargs -r docker volume rm 2>/dev/null || true
                    docker volume prune -f 2>/dev/null || true
                    success "Volumes cleaned!"
                    ;;
                --symlink|-s)
                    if [[ -L "$LINK_TARGET" ]]; then
                        rm -f "$LINK_TARGET"
                        success "Removed claudebox symlink from $(dirname "$LINK_TARGET")"
                    else
                        info "No claudebox symlink found at $LINK_TARGET"
                    fi
                    exit 0
                    ;;
                --dangling|-d)
                    warn "Removing dangling images and unused containers..."
                    docker image prune -f
                    docker container prune -f
                    success "Dangling resources cleaned!"
                    ;;
                --logs|-l)
                    warn "Clearing Docker container logs..."
                    docker ps -a --filter "ancestor=$IMAGE_NAME" -q | while read -r container; do
                        docker logs "$container" >/dev/null 2>&1 && echo -n | docker logs "$container" 2>/dev/null || true
                    done
                    success "Container logs cleared!"
                    ;;
                --project|-p)
                    warn "Removing data for current project: $PROJECT_DIR"
                    local profile_file
                    profile_file=$(get_profile_file_path)

                    # Remove profile
                    if [[ -f "$profile_file" ]]; then
                        rm -f "$profile_file"
                        success "Removed profile for $PROJECT_DIR"
                    fi

                    # Remove project-specific folder
                    local project_folder_name
                    project_folder_name=$(get_project_folder_name "$PROJECT_DIR")
                    local project_claudebox_dir="$HOME/.claudebox/$project_folder_name"

                    if [[ -d "$project_claudebox_dir" ]]; then
                        rm -rf "$project_claudebox_dir"
                        success "Removed project data folder: $project_claudebox_dir"
                    else
                        info "No project data folder found"
                    fi
                    exit 0
                    ;;
                *)
                    warn "Cleaning ClaudeBox containers..."
                    # Remove all profile files
                    rm -rf "$HOME/.claudebox/profiles"
                    # Remove any containers from this image
                    docker ps -a --filter "ancestor=$IMAGE_NAME" -q | xargs -r docker rm -f 2>/dev/null || true
                    # Remove orphaned containers from images that no longer exist
                    # This is safer as it only removes containers whose images are gone
                    docker ps -a --filter "status=exited" --format "{{.ID}} {{.Image}}" | while read id image; do
                        if ! docker image inspect "$image" >/dev/null 2>&1; then
                            docker rm -f "$id" 2>/dev/null || true
                        fi
                    done
                    success "Containers removed! Image preserved for quick recreation."
                    ;;
            esac
            echo
            docker system df
            exit 0
            ;;

        help|--help|-h)
            if docker image inspect "$IMAGE_NAME" &>/dev/null; then
                docker run --rm \
                    -u "$DOCKER_USER" \
                    --entrypoint /home/$DOCKER_USER/claude-wrapper \
                    "$IMAGE_NAME" --help | sed '1s/claude/claudebox/g'
                echo
                cecho "Added Options:" "$WHITE"
                echo -e "${CYAN}  -v, --verbose                   ${WHITE}Show detailed output"
                echo -e "${CYAN}  --dangerously-enable-sudo       ${WHITE}Enable sudo without password"
                echo
                cecho "Added Commands:" "$WHITE"
                echo -e "  profile [names...]              Install language profiles"
                echo -e "  install <packages>              Install apt packages"
                echo -e "  shell                           Open bash shell in container"
                echo -e "  info                            Show ClaudeBox container status"
                echo -e "  clean                           Clean up ClaudeBox resources (use 'clean --help' for options)"
                echo -e "  rebuild                         Rebuild the Docker image from scratch${NC}"
            else
                cecho "ClaudeBox - Claude Code Docker Environment" "$CYAN"
                echo
                warn "First run setup required!"
                echo "Run script without arguments first to build the Docker image."
            fi
            exit 0
            ;;

        info)
            shift
            cecho "ClaudeBox Profile Status" "$CYAN"
            echo

            local profile_dir="$HOME/.claudebox/profiles"
            if [[ ! -d "$profile_dir" ]] || [[ -z "$(ls -A "$profile_dir" 2>/dev/null)" ]]; then
                warn "No profiles configured yet."
                exit 0
            fi

            # Show all profiles
            local count
            count=$(ls -1 "$profile_dir"/*.ini 2>/dev/null | wc -l)
            info "Tracking $count project profile(s)"
            echo

            # Show each project's profiles
            for pfile in "$profile_dir"/*.ini; do
                [[ -f "$pfile" ]] || continue
                local proj_path
                proj_path=$(basename "$pfile" .ini | sed 's|-|/|g')
                cecho "/$proj_path:" "$YELLOW"

                # Show profiles
                local profiles=()
                readarray -t profiles < <(read_profile_section "$pfile" "profiles")
                if [[ ${#profiles[@]} -gt 0 ]]; then
                    echo "  Profiles: ${profiles[*]}"
                fi

                # Show packages
                local packages=()
                readarray -t packages < <(read_profile_section "$pfile" "packages")
                if [[ ${#packages[@]} -gt 0 ]]; then
                    echo "  Packages: ${packages[*]}"
                fi
                echo
            done

            # Show current project's profiles
            local current_profile_file
            current_profile_file=$(get_profile_file_path)
            if [[ -f "$current_profile_file" ]]; then
                cecho "Current project ($PROJECT_DIR):" "$GREEN"
                local current_profiles=()
                readarray -t current_profiles < <(read_profile_section "$current_profile_file" "profiles")
                if [[ ${#current_profiles[@]} -gt 0 ]]; then
                    echo "  Profiles: ${current_profiles[*]}"
                fi

                local current_packages=()
                readarray -t current_packages < <(read_profile_section "$current_profile_file" "packages")
                if [[ ${#current_packages[@]} -gt 0 ]]; then
                    echo "  Packages: ${current_packages[*]}"
                fi
            else
                info "Current project has no profiles configured."
            fi

            # Show running containers
            echo
            local running_containers
            running_containers=$(docker ps --filter "ancestor=$IMAGE_NAME" --format "table {{.ID}}\t{{.Status}}\t{{.Command}}" | tail -n +2)
            if [[ -n "$running_containers" ]]; then
                cecho "Running ClaudeBox containers:" "$YELLOW"
                echo "$running_containers"
            else
                info "No ClaudeBox containers currently running."
            fi

            exit 0
            ;;

        rebuild)
            shift  # Remove 'rebuild' from arguments
            warn "Rebuilding ClaudeBox Docker image..."

            # Check if image exists and remove it
            if docker image inspect "$IMAGE_NAME" &>/dev/null; then
                # Stop any running containers from this image first
                docker ps -a --filter "ancestor=$IMAGE_NAME" -q | xargs -r docker rm -f 2>/dev/null || true
                docker rmi -f "$IMAGE_NAME" 2>/dev/null || true
            fi

            # Continue to main execution which will detect missing image and rebuild
            ;;
    esac

    # Setup workspace and global config
    setup_project_folder
    setup_global_mcp_config
    setup_claude_agent_command

    # Profile tracking
    mkdir -p "$HOME/.claudebox/profiles"

    # Check if we need to rebuild based on profiles
    local need_rebuild=false
    local current_profiles=()
    local profile_hash=""

    # Collect all profiles from all projects
    if [[ -d "$HOME/.claudebox/profiles" ]]; then
        for profile_file in "$HOME/.claudebox/profiles"/*.ini; do
            [[ -f "$profile_file" ]] || continue
            local profiles_from_file=()
            readarray -t profiles_from_file < <(read_profile_section "$profile_file" "profiles")
            for profile in "${profiles_from_file[@]}"; do
                profile=$(echo "$profile" | tr -d '[:space:]')
                [[ -z "$profile" ]] && continue
                # Add to array if not already present
                local found=false
                for p in "${current_profiles[@]}"; do
                    [[ "$p" == "$profile" ]] && found=true && break
                done
                [[ "$found" == "false" ]] && current_profiles+=("$profile")
            done
        done

        # Create a hash of current profiles
        if [[ ${#current_profiles[@]} -gt 0 ]]; then
            profile_hash=$(printf '%s\n' "${current_profiles[@]}" | sort | shasum -a 256 | cut -d' ' -f1)
        fi
    fi

    # Check if image exists and if profiles match
    if docker image inspect "$IMAGE_NAME" >/dev/null 2>&1; then
        # Get the profile hash from the image labels
        local image_profile_hash
        image_profile_hash=$(docker inspect "$IMAGE_NAME" --format '{{index .Config.Labels "claudebox.profiles"}}' 2>/dev/null || echo "")

        if [[ "$profile_hash" != "$image_profile_hash" ]]; then
            warn "Profiles have changed. Rebuilding image..."
            warn "Current profiles: ${current_profiles[*]}"
            docker rmi -f "$IMAGE_NAME" 2>/dev/null || true
            need_rebuild=true
        fi
    else
        need_rebuild=true
    fi

    # Build image if needed
    if [[ "$need_rebuild" == "true" ]] || ! docker image inspect "$IMAGE_NAME" >/dev/null 2>&1; then
        logo
        local dockerfile
        dockerfile=$(mktemp /tmp/claudebox-dockerfile.XXXXXX)
        trap "rm -f '$dockerfile'" EXIT

        cat > "$dockerfile" <<'DOCKERFILE'
FROM debian:bookworm
ARG USER_ID GROUP_ID USERNAME NODE_VERSION

RUN echo '#!/bin/sh\nexit 101' > /usr/sbin/policy-rc.d && chmod +x /usr/sbin/policy-rc.d

# Install locales first to fix locale warnings
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq locales && \
    echo "en_US.UTF-8 UTF-8" > /etc/locale.gen && \
    locale-gen en_US.UTF-8 && \
    rm -rf /var/lib/apt/lists/*

# Set locale environment variables
ENV LANG=en_US.UTF-8 \
    LANGUAGE=en_US:en \
    LC_ALL=en_US.UTF-8

RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq curl gnupg ca-certificates sudo git && \
    apt-get clean

RUN groupadd -g $GROUP_ID $USERNAME || true && \
    useradd -m -u $USER_ID -g $GROUP_ID -s /bin/bash $USERNAME

# Persist bash history
RUN SNIPPET="export PROMPT_COMMAND='history -a' && export HISTFILE=/commandhistory/.bash_history" \
    && mkdir /commandhistory \
    && touch /commandhistory/.bash_history \
    && chown -R $USERNAME /commandhistory

RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq \
    build-essential git wget curl unzip file vim nano \
    jq make less rsync openssh-client \
    procps sudo fzf zsh man-db gnupg2 \
    && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

RUN curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg && \
    chmod go+r /usr/share/keyrings/githubcli-archive-keyring.gpg && \
    echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | tee /etc/apt/sources.list.d/github-cli.list > /dev/null && \
    apt update -qq && \
    apt install gh -y -qq && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

RUN DELTA_VERSION=$(curl -s https://api.github.com/repos/dandavison/delta/releases/latest | grep -Po '"tag_name": "\K[^"]*') && \
    ARCH=$(dpkg --print-architecture) && \
    wget -q https://github.com/dandavison/delta/releases/download/${DELTA_VERSION}/git-delta_${DELTA_VERSION}_${ARCH}.deb && \
    dpkg -i git-delta_${DELTA_VERSION}_${ARCH}.deb && \
    rm git-delta_${DELTA_VERSION}_${ARCH}.deb

USER $USERNAME
WORKDIR /home/$USERNAME

RUN sh -c "$(wget -O- https://github.com/deluan/zsh-in-docker/releases/download/v1.2.0/zsh-in-docker.sh)" -- \
    -p git \
    -p fzf \
    -a "source /usr/share/doc/fzf/examples/key-bindings.zsh" \
    -a "source /usr/share/doc/fzf/examples/completion.zsh" \
    -a "export PROMPT_COMMAND='history -a' && export HISTFILE=/commandhistory/.bash_history" \
    -a 'export HISTFILE=$HOME/.history/.zsh_history' \
    -a 'export HISTSIZE=10000' \
    -a 'export SAVEHIST=10000' \
    -a 'setopt HIST_IGNORE_DUPS' \
    -a 'setopt SHARE_HISTORY' \
    -a 'mkdir -p $HOME/.history' \
    -a 'export NVM_DIR="$HOME/.nvm"' \
    -a '[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"' \
    -a '[ -s "$NVM_DIR/bash_completion" ] && \. "$NVM_DIR/bash_completion"' \
    -x

RUN curl -LsSf https://astral.sh/uv/install.sh | sh
RUN echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/.bashrc && \
    echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/.zshrc

RUN git config --global core.pager delta && \
    git config --global interactive.diffFilter "delta --color-only" && \
    git config --global delta.navigate true && \
    git config --global delta.light false && \
    git config --global delta.side-by-side true

# Set DEVCONTAINER environment variable to help with orientation
ENV DEVCONTAINER=true

# Set the default shell to zsh rather than sh
ENV SHELL=/bin/zsh

ENV NVM_DIR="/home/$USERNAME/.nvm"
RUN curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | bash

RUN bash -c "source $NVM_DIR/nvm.sh && \
    if [[ \"$NODE_VERSION\" == '--lts' ]]; then \
        nvm install --lts && \
        nvm alias default 'lts/*'; \
    else \
        nvm install $NODE_VERSION && \
        nvm alias default $NODE_VERSION; \
    fi && \
    nvm use default"

RUN echo 'export NVM_DIR="$HOME/.nvm"' >> ~/.bashrc && \
    echo '[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"' >> ~/.bashrc && \
    echo '[ -s "$NVM_DIR/bash_completion" ] && \. "$NVM_DIR/bash_completion"' >> ~/.bashrc

RUN bash -c "source $NVM_DIR/nvm.sh && \
    nvm use default && \
    npm install -g @anthropic-ai/claude-code"


# Install profile packages as separate layers for better caching
USER root
DOCKERFILE

        # Add profile installations based on current profiles
        if [[ ${#current_profiles[@]} -gt 0 ]]; then
            info "Building with profiles: ${current_profiles[*]}"
            for profile in "${current_profiles[@]}"; do
                case "$profile" in
                    c)
                        cat >> "$dockerfile" <<'DOCKERFILE'
# C/C++ Development Profile
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq build-essential gcc g++ gdb valgrind cmake ninja-build clang clang-format clang-tidy cppcheck doxygen libboost-all-dev autoconf automake libtool pkg-config libcmocka-dev libcmocka0 lcov libncurses5-dev libncursesw5-dev && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

DOCKERFILE
                        ;;
                    python)
    cat >> "$dockerfile" <<'DOCKERFILE'
# Python Development Profile
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq python3 python3-pip python3-venv python3-dev build-essential libffi-dev libssl-dev && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

DOCKERFILE
                        ;;
                    rust)
                        cat >> "$dockerfile" <<'DOCKERFILE'
# Rust Development Profile
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq curl build-essential pkg-config libssl-dev && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

USER $USERNAME
RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y && \
    echo 'source $HOME/.cargo/env' >> ~/.bashrc && \
    echo 'source $HOME/.cargo/env' >> ~/.zshrc
USER root

DOCKERFILE
                        ;;
                    go)
                        cat >> "$dockerfile" <<'DOCKERFILE'
# Go Development Profile
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq wget git build-essential && \
    apt-get clean && rm -rf /var/lib/apt/lists/* && \
    GO_VERSION="1.21.5" && \
    wget -q "https://go.dev/dl/go${GO_VERSION}.linux-amd64.tar.gz" && \
    tar -C /usr/local -xzf "go${GO_VERSION}.linux-amd64.tar.gz" && \
    rm "go${GO_VERSION}.linux-amd64.tar.gz" && \
    echo 'export PATH=$PATH:/usr/local/go/bin' >> /etc/profile.d/go.sh

DOCKERFILE
                        ;;
                    javascript)
                        cat >> "$dockerfile" <<'DOCKERFILE'
# JavaScript Development Profile
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq build-essential python3 && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

DOCKERFILE
                        ;;
                    openwrt)
                        cat >> "$dockerfile" <<'DOCKERFILE'
# OpenWRT Development Profile
RUN export DEBIAN_FRONTEND=noninteractive && \
    apt-get update -qq && \
    apt-get install -y -qq build-essential gcc g++ make git wget unzip sudo file python3 python3-distutils rsync libncurses5-dev zlib1g-dev gawk gettext libssl-dev xsltproc libelf-dev libtool automake autoconf ccache subversion swig time qemu-system-arm qemu-system-aarch64 qemu-system-mips qemu-system-x86 qemu-utils && \
    apt-get clean && rm -rf /var/lib/apt/lists/*

DOCKERFILE
                        ;;
                    *)
                        warn "Unknown profile: $profile"
                        ;;
                esac
            done
        fi

        # Add label with profile hash
        echo "# Label the image with the profile hash for change detection" >> "$dockerfile"
        echo "LABEL claudebox.profiles=\"$profile_hash\"" >> "$dockerfile"
        echo "" >> "$dockerfile"
        cat >> "$dockerfile" <<'DOCKERFILE'

USER $USERNAME

RUN bash -c "source $NVM_DIR/nvm.sh && claude --version"

RUN echo '#!/bin/bash' > ~/claude-wrapper && \
   echo 'export NVM_DIR="$HOME/.nvm"' >> ~/claude-wrapper && \
   echo '[ -s "$NVM_DIR/nvm.sh" ] && \. "$NVM_DIR/nvm.sh"' >> ~/claude-wrapper && \
   echo 'nvm use default >/dev/null 2>&1' >> ~/claude-wrapper && \
   echo '# Load PATH from bashrc for tools like uv' >> ~/claude-wrapper && \
   echo 'export PATH="$HOME/.local/bin:$PATH"' >> ~/claude-wrapper && \
   echo '# Use MCP config if it exists' >> ~/claude-wrapper && \
   echo 'if [ -f "$HOME/.claudebox/.mcp.json" ]; then' >> ~/claude-wrapper && \
   echo '    exec claude --mcp-config "$HOME/.claudebox/.mcp.json" "$@"' >> ~/claude-wrapper && \
   echo 'else' >> ~/claude-wrapper && \
   echo '    exec claude "$@"' >> ~/claude-wrapper && \
   echo 'fi' >> ~/claude-wrapper && \
   chmod +x ~/claude-wrapper

WORKDIR /workspace

USER root
COPY <<ENTRYPOINT /usr/local/bin/docker-entrypoint
#!/bin/bash
ENABLE_SUDO=false
while [[ \$# -gt 0 ]]; do
    case "\$1" in
        --dangerously-enable-sudo) ENABLE_SUDO=true; shift ;;
        *) break ;;
    esac
done

if [ -d /home/DOCKERUSER/.claudebox-project ]; then
    mkdir -p /home/DOCKERUSER/.claudebox-project/memory
    chown -R DOCKERUSER /home/DOCKERUSER/.claudebox-project
fi

if [ "\$ENABLE_SUDO" = "true" ]; then
    echo "DOCKERUSER ALL=(ALL) NOPASSWD:ALL" > /etc/sudoers.d/DOCKERUSER
    chmod 0440 /etc/sudoers.d/DOCKERUSER
fi

# Configure Git user settings if provided
if [ -n "\$GIT_USER_NAME" ]; then
    su - DOCKERUSER -c "git config --global user.name '\$GIT_USER_NAME'"
fi
if [ -n "\$GIT_USER_EMAIL" ]; then
    su - DOCKERUSER -c "git config --global user.email '\$GIT_USER_EMAIL'"
fi

if [ -n "\$CLAUDEBOX_PROJECT_DIR" ]; then
    PROFILE_FILE="/home/DOCKERUSER/.claudebox/profiles/\$(echo "\$CLAUDEBOX_PROJECT_DIR" | sed 's|/|-|g' | sed 's|^-||').ini"

    if command -v uv >/dev/null 2>&1 && [ -f "\$PROFILE_FILE" ] && grep -q "python" "\$PROFILE_FILE"; then
        if [ ! -d /home/DOCKERUSER/.claudebox-project/.venv ]; then
            su - DOCKERUSER -c "uv venv /home/DOCKERUSER/.claudebox-project/.venv"
            if [ -f /workspace/pyproject.toml ]; then
                su - DOCKERUSER -c "cd /workspace && uv sync"
            else
                su - DOCKERUSER -c "uv pip install --python /home/DOCKERUSER/.claudebox-project/.venv/bin/python ipython black pylint mypy flake8 pytest ruff"
            fi
        fi

        for shell_rc in /home/DOCKERUSER/.zshrc /home/DOCKERUSER/.bashrc; do
            if ! grep -q "source /home/DOCKERUSER/.claudebox-project/.venv/bin/activate" "\$shell_rc"; then
                echo 'if [ -f /home/DOCKERUSER/.claudebox-project/.venv/bin/activate ]; then source /home/DOCKERUSER/.claudebox-project/.venv/bin/activate; fi' >> "\$shell_rc"
            fi
        done
    fi
fi

cd /home/DOCKERUSER
# Check if --shell-mode is anywhere in the arguments
SHELL_MODE=false
CLAUDE_ARGS=()
for arg in "\$@"; do
    if [[ "\$arg" == "--shell-mode" ]]; then
        SHELL_MODE=true
    else
        CLAUDE_ARGS+=("\$arg")
    fi
done

if [[ "\$SHELL_MODE" == "true" ]]; then
    exec su DOCKERUSER -c "source /home/DOCKERUSER/.nvm/nvm.sh && export PATH=\"/home/DOCKERUSER/.local/bin:\$PATH\" && cd /workspace && exec /bin/zsh"
else
    cmd="cd /workspace && /home/DOCKERUSER/claude-wrapper"
    for arg in "\${CLAUDE_ARGS[@]}"; do
        cmd="\$cmd '\$arg'"
    done
    exec su DOCKERUSER -c "\$cmd"
fi
ENTRYPOINT

RUN sed -i "s/DOCKERUSER/$USERNAME/g" /usr/local/bin/docker-entrypoint && \
    chmod +x /usr/local/bin/docker-entrypoint

ENTRYPOINT ["/usr/local/bin/docker-entrypoint"]
DOCKERFILE

        run_docker_build "$dockerfile"

        echo -e "\n${GREEN}Complete!${NC}\n"
        success "Docker image '$IMAGE_NAME' built!"

        echo
        cecho "ClaudeBox Setup Complete!" "$CYAN"
        echo
        cecho "Quick Start:" "$GREEN"
        echo -e "  ${YELLOW}claudebox [options]${NC}        # Launch Claude CLI"
        echo
        cecho "Power Features:" "$GREEN"
        echo -e "  ${YELLOW}claudebox profile${NC}                # See all language profiles"
        echo -e "  ${YELLOW}claudebox profile c openwrt${NC}      # Install C + OpenWRT tools"
        echo -e "  ${YELLOW}claudebox profile python ml${NC}      # Install Python + ML stack"
        echo -e "  ${YELLOW}claudebox install <packages>${NC}     # Install additional apt packages"
        echo -e "  ${YELLOW}claudebox shell${NC}                  # Open bash shell in container"
        echo
        cecho "Security:" "$GREEN"
        echo -e "  Sudo access: ON by default for development"
        echo -e "  Network firewall: OFF (full internet access)"
        echo
        cecho "Maintenance:" "$GREEN"
        echo -e "  ${YELLOW}claudebox clean --help${NC}            # See all cleanup options"
        echo
        cecho "Just install the profile you need and start coding!" "$PURPLE"
       exit 0
   fi

   # Run container
   local extra_mounts=()

   # Ensure .claudebox exists with proper permissions
   if [[ ! -d "$HOME/.claudebox" ]]; then
       mkdir -p "$HOME/.claudebox"
   fi

   # Fix permissions if needed
   if [[ ! -w "$HOME/.claudebox" ]]; then
       warn "Fixing .claudebox permissions..."
       sudo chown -R "$USER:$USER" "$HOME/.claudebox" || true
   fi


   # Create new container with --rm
   info "Starting claudebox for $PROJECT_DIR..."
   
   # Show Git configuration status
   local git_user_name git_user_email
   git_user_name=$(git config --get user.name 2>/dev/null || echo '')
   git_user_email=$(git config --get user.email 2>/dev/null || echo '')
   
   if [[ -n "$git_user_name" && -n "$git_user_email" ]]; then
       info "Git configuration: $git_user_name <$git_user_email>"
   elif [[ -n "$git_user_name" ]]; then
       info "Git configuration: $git_user_name (no email configured)"
   elif [[ -n "$git_user_email" ]]; then
       info "Git configuration: $git_user_email (no name configured)"
   else
       warn "No Git user configuration found - commits will use Claude Code defaults"
   fi

   # Check if we have a TTY
   local tty_flag=""
   if [ -t 0 ] && [ -t 1 ]; then
       tty_flag="-it"
   else
       tty_flag="-i"
   fi

   # Get project-specific folder
   local project_folder_name
   project_folder_name=$(get_project_folder_name "$PROJECT_DIR")

   # Filter out ClaudeBox-specific flags before passing to Docker entrypoint
   # Note: --shell-mode should be passed through since it's handled by the entrypoint
   FILTERED_ARGS=()
   for arg in "${RUN_ARGS[@]:-$@}"; do
       case "$arg" in
           --dangerously-enable-sudo)
               # Skip flags that are handled by ClaudeBox itself
               ;;
           *)
               FILTERED_ARGS+=("$arg")
               ;;
       esac
   done

   docker run $tty_flag --rm \
       -w /workspace \
       -v "$PROJECT_DIR":/workspace \
       -v "$HOME/.claudebox/$project_folder_name":/home/$DOCKER_USER/.claudebox-project \
       -v "$HOME/.claudebox":/home/$DOCKER_USER/.claudebox \
       -v "$HOME/.claude.json":/home/$DOCKER_USER/.claude.json \
       -v "$HOME/.claude":/home/$DOCKER_USER/.claude \
       -v "$HOME/.npmrc":/home/$DOCKER_USER/.npmrc:ro \
       -v "$HOME/.ssh":/home/$DOCKER_USER/.ssh:ro \
       -e "NODE_ENV=${NODE_ENV:-production}" \
       -e "ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}" \
       -e "NODE_OPTIONS=--max-old-space-size=4096" \
       -e "POWERLEVEL9K_DISABLE_GITSTATUS=true" \
       -e "CLAUDEBOX_PROJECT_DIR=$PROJECT_DIR" \
       -e "GIT_USER_NAME=$(git config --get user.name 2>/dev/null || echo '')" \
       -e "GIT_USER_EMAIL=$(git config --get user.email 2>/dev/null || echo '')" \
       "${extra_mounts[@]}" \
       "$IMAGE_NAME" \
       ${CLAUDEBOX_ENABLE_SUDO:+--dangerously-enable-sudo} \
       "${DEFAULT_FLAGS[@]}" "${FILTERED_ARGS[@]}"
}

main "$@"
